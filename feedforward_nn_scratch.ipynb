{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0c8f2f-7959-479e-9456-1fa2c24d303b",
   "metadata": {},
   "source": [
    "## Basic Feedforward Neural Network From Scratch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b36588-1fe6-46db-a025-a47b9575c339",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f006543-c3c3-4441-bd00-a3ec86788ee4",
   "metadata": {},
   "source": [
    "This report aims to build a fully custom feedforward neural network framework using Python and NumPy, which will be accomplished namely by:\n",
    "\n",
    "1. Implementing core components such as layers, activation functions, loss functions, and an optimizer\n",
    "2. Demonstrating training on a sample dataset, visualize performance, and compare results against a baseline\n",
    "3. Exploring advanced features like different optimizers, regularization, and potential improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf86ee-b19e-4754-84cd-7b335a3964d6",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "---\n",
    "##### 1. Imports and Setup\n",
    "##### 2. Core Implementation\n",
    "##### $~~~~~$ 2.1 Architecture and Data Structures\n",
    "##### $~~~~~$ 2.2 Building the Model\n",
    "##### 3. Data Preparation\n",
    "##### 4. Training and Evaluation\n",
    "##### 5. Hyperparameter Tuning and Results\n",
    "##### $~~~~~$ 5.1 Grid Search / Random Search (Conceptual)\n",
    "##### 6. Advanced Features\n",
    "##### $~~~~~$ 6.1 Alternative Optimizers (Adam, RMSProp, etc.)\n",
    "##### $~~~~~$ 6.2 Batch Normalization and Dropout\n",
    "##### 7. Conclusions and Future Work\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88641f47-ab0e-457a-92cc-318c2fc47321",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdf748a-e542-4f5f-936b-8b064e8887a0",
   "metadata": {},
   "source": [
    "Below, we load the necessary Python libraries for matrix operations, plotting, and performance tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a51820-f271-4ec0-8d92-035fcf91b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for repproducibility \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e0545-42fe-4e72-a835-49032c7adbf9",
   "metadata": {},
   "source": [
    "## 2. Core Implementation\n",
    "\n",
    "### 2.1 Architecture and Data Structures\n",
    "\n",
    "We will implement a modular, object-oriented design. Each component—Layer, Activation, Loss, and Optimizer—will have clear responsibilities. This allows easy extension and maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60341729-e852-4406-9f7d-3ca63285f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X  # store for backward pass\n",
    "        return np.dot(X, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.01, reg_lambda=0.0):\n",
    "        m = self.X.shape[0]\n",
    "        dW = (1/m) * np.dot(self.X.T, dZ) + reg_lambda * self.weights\n",
    "        db = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "        \n",
    "        # gradient for next layer\n",
    "        dX = np.dot(dZ, self.weights.T)\n",
    "        \n",
    "        # update weights\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.biases -= learning_rate * db\n",
    "        \n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e549bc-5d5c-478b-95f9-65073323f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def forward(self, Z):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, dA, Z):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ReLU(Activation):\n",
    "    def forward(self, Z):\n",
    "        self.Z = Z\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def backward(self, dA, Z=None):\n",
    "        # If we haven't stored Z, use self.Z\n",
    "        Z = self.Z if Z is None else Z\n",
    "        return dA * (Z > 0)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def forward(self, Z):\n",
    "        self.A = 1 / (1 + np.exp(-Z))\n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dA, Z=None):\n",
    "        # Use stored output A = sigmoid(Z)\n",
    "        A = self.A\n",
    "        return dA * A * (1 - A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46da3b-9d28-4425-9499-77dd1d945264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MeanSquaredError(LossFunction):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return np.mean(0.5 * (y_true - y_pred)**2)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        return (y_pred - y_true) / y_true.shape[0]\n",
    "\n",
    "class BinaryCrossEntropy(LossFunction):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        eps = 1e-8\n",
    "        return -np.mean(y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps))\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        eps = 1e-8\n",
    "        return -(y_true / (y_pred + eps) - (1 - y_true) / (1 - y_pred + eps)) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f2ebd-dc88-4a5e-b92f-7ca3fb22a390",
   "metadata": {},
   "source": [
    "### 2.2 Building the Model\n",
    "\n",
    "We create a `NeuralNetwork` class to combine layers and activations into a cohesive model. This class will handle:\n",
    "- Forward propagation through all layers\n",
    "- Calculating the loss\n",
    "- Backward propagation to update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2767af7-ea65-4cef-86df-af3c5ef5ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activations, loss_func, learning_rate=0.01, reg_lambda=0.0):\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.loss_func = loss_func\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            Z = layer.forward(out)\n",
    "            if i < len(self.activations):\n",
    "                out = self.activations[i].forward(Z)\n",
    "            else:\n",
    "                out = Z\n",
    "        return out\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        return self.loss_func.forward(y_pred, y_true)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        dA = self.loss_func.backward(y_pred, y_true)\n",
    "        \n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            if i < len(self.activations):\n",
    "                dZ = self.activations[i].backward(dA)\n",
    "            else:\n",
    "                dZ = dA\n",
    "            dA = self.layers[i].backward(dZ, self.learning_rate, self.reg_lambda)\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, verbose=True):\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            history.append(loss)\n",
    "            \n",
    "            # Backprop\n",
    "            self.backward(y_pred, y)\n",
    "            \n",
    "            if verbose and (epoch+1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        prob = self.forward(X)\n",
    "        return (prob >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d03250-357a-4284-960f-19b8d3dd147e",
   "metadata": {},
   "source": [
    "### 3. Data Preparation\n",
    "\n",
    "For demonstration, let's create a synthetic binary classification dataset. In a real project, you might load data from an external source, perform cleaning, normalizing, and potentially augmenting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7dc156-9fbf-4659-b1ed-f59b0e403954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N = 1000\n",
    "X_data = np.random.randn(N, 2)\n",
    "y_data = (X_data[:, 0] + X_data[:, 1] > 0).astype(int).reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train set size:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set size:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba1adb-bcf2-49c1-a71a-db15d75e9da9",
   "metadata": {},
   "source": [
    "### 4. Training and Evaluation\n",
    "\n",
    "We construct a small feedforward network with one hidden layer and a sigmoid output for binary classification. We'll use the Binary Cross-Entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468953f0-59c1-4d44-9c38-a58333cfbd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = DenseLayer(input_dim=2, output_dim=4)\n",
    "act1 = ReLU()\n",
    "\n",
    "layer2 = DenseLayer(input_dim=4, output_dim=1)\n",
    "act2 = Sigmoid()\n",
    "\n",
    "loss_fn = BinaryCrossEntropy()\n",
    "\n",
    "network = NeuralNetwork(\n",
    "    layers=[layer1, layer2],\n",
    "    activations=[act1, act2],\n",
    "    loss_func=loss_fn,\n",
    "    learning_rate=0.05,\n",
    "    reg_lambda=0.01\n",
    ")\n",
    "\n",
    "history = network.fit(X_train, y_train, epochs=200, verbose=True)\n",
    "\n",
    "plt.plot(history)\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred_test = network.predict(X_test)\n",
    "accuracy = (y_pred_test == y_test).mean()\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a73f33-6065-444c-80b6-ffe1d37adeef",
   "metadata": {},
   "source": [
    "### 5. Hyperparameter Tuning and Results\n",
    "\n",
    "#### 5.1 Grid Search / Random Search (Conceptual)\n",
    "\n",
    "While we won’t execute an extensive hyperparameter search here, your project could include a systematic procedure to explore:\n",
    "- Learning rates (e.g., 0.01, 0.05, 0.1)\n",
    "- Regularization parameters (0, 0.001, 0.01, 0.1)\n",
    "- Different numbers of hidden units\n",
    "- Batch sizes\n",
    "\n",
    "You could automate experiments and store results for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba1234-aaff-4fe5-b427-b051eac5bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not complete\n",
    "learning_rates = [0.01, 0.05, 0.1]\n",
    "reg_lambdas = [0.0, 0.01, 0.1]\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for rl in reg_lambdas:\n",
    "        model = NeuralNetwork(layers=..., activations=..., loss_func=..., learning_rate=lr, reg_lambda=rl)\n",
    "        history = model.fit(X_train, y_train, epochs=100, verbose=False)\n",
    "        acc = (model.predict(X_test) == y_test).mean()\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_params = (lr, rl)\n",
    "\n",
    "print(\"Best Accuracy:\", best_acc)\n",
    "print(\"Best Params (LR, REG):\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8636402b-1605-4ed8-b8f6-d8484e5124ec",
   "metadata": {},
   "source": [
    "### 6. Advanced Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da9b7e-3ca1-4cb4-9b0d-b61b0c0055c3",
   "metadata": {},
   "source": [
    "#### 6.1 Alternative Optimizers (Adam, RMSProp, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c34772-dc32-4e80-aa0d-5d585c436728",
   "metadata": {},
   "source": [
    "#### 6.2 Batch Normalization and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee07b2b-298a-44b2-bd8a-f1c6ee4efd32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffnn(scratch)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
