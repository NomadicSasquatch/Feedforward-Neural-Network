{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0c8f2f-7959-479e-9456-1fa2c24d303b",
   "metadata": {},
   "source": [
    "## Basic Feedforward Neural Network From Scratch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b36588-1fe6-46db-a025-a47b9575c339",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f006543-c3c3-4441-bd00-a3ec86788ee4",
   "metadata": {},
   "source": [
    "This report aims to build a fully custom feedforward neural network framework using Python and NumPy, which will be accomplished namely by:\n",
    "\n",
    "1. Implementing core components such as layers, activation functions, loss functions, and an optimizer\n",
    "2. Demonstrating training on a sample dataset, visualize performance, and compare results against a baseline\n",
    "3. Exploring advanced features like different optimizers, regularization, and potential improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf86ee-b19e-4754-84cd-7b335a3964d6",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "---\n",
    "##### 1. Imports and Setup\n",
    "##### 2. Core Implementation\n",
    "##### $~~~~~$ 2.1 Architecture and Data Structures\n",
    "##### $~~~~~$ 2.2 Building the Model\n",
    "##### 3. Data Preparation  \n",
    "##### 4. Training and Evaluation  \n",
    "##### 5. Hyperparameter Tuning and Results  \n",
    "##### 6. Advanced Features  \n",
    "##### 7. Conclusions and Future Work\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88641f47-ab0e-457a-92cc-318c2fc47321",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdf748a-e542-4f5f-936b-8b064e8887a0",
   "metadata": {},
   "source": [
    "Below, we load the necessary Python libraries for matrix operations, plotting, and performance tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a51820-f271-4ec0-8d92-035fcf91b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for repproducibility \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e0545-42fe-4e72-a835-49032c7adbf9",
   "metadata": {},
   "source": [
    "## 2. Core Implementation\n",
    "\n",
    "### 2.1 Architecture and Data Structures\n",
    "\n",
    "We will implement a modular, object-oriented design. Each component—Layer, Activation, Loss, and Optimizer—will have clear responsibilities. This allows easy extension and maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60341729-e852-4406-9f7d-3ca63285f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X  # store for backward pass\n",
    "        return np.dot(X, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.01, reg_lambda=0.0):\n",
    "        m = self.X.shape[0]\n",
    "        dW = (1/m) * np.dot(self.X.T, dZ) + reg_lambda * self.weights\n",
    "        db = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "        \n",
    "        # gradient for next layer\n",
    "        dX = np.dot(dZ, self.weights.T)\n",
    "        \n",
    "        # update weights\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.biases -= learning_rate * db\n",
    "        \n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e549bc-5d5c-478b-95f9-65073323f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def forward(self, Z):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, dA, Z):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ReLU(Activation):\n",
    "    def forward(self, Z):\n",
    "        self.Z = Z\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def backward(self, dA, Z=None):\n",
    "        # If we haven't stored Z, use self.Z\n",
    "        Z = self.Z if Z is None else Z\n",
    "        return dA * (Z > 0)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def forward(self, Z):\n",
    "        self.A = 1 / (1 + np.exp(-Z))\n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dA, Z=None):\n",
    "        # Use stored output A = sigmoid(Z)\n",
    "        A = self.A\n",
    "        return dA * A * (1 - A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46da3b-9d28-4425-9499-77dd1d945264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MeanSquaredError(LossFunction):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return np.mean(0.5 * (y_true - y_pred)**2)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        return (y_pred - y_true) / y_true.shape[0]\n",
    "\n",
    "class BinaryCrossEntropy(LossFunction):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        eps = 1e-8\n",
    "        return -np.mean(y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps))\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        eps = 1e-8\n",
    "        return -(y_true / (y_pred + eps) - (1 - y_true) / (1 - y_pred + eps)) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f2ebd-dc88-4a5e-b92f-7ca3fb22a390",
   "metadata": {},
   "source": [
    "### 2.2 Building the Model\n",
    "\n",
    "We create a `NeuralNetwork` class to combine layers and activations into a cohesive model. This class will handle:\n",
    "- Forward propagation through all layers\n",
    "- Calculating the loss\n",
    "- Backward propagation to update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2767af7-ea65-4cef-86df-af3c5ef5ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activations, loss_func, learning_rate=0.01, reg_lambda=0.0):\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.loss_func = loss_func\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            Z = layer.forward(out)\n",
    "            if i < len(self.activations):\n",
    "                out = self.activations[i].forward(Z)\n",
    "            else:\n",
    "                out = Z\n",
    "        return out\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        return self.loss_func.forward(y_pred, y_true)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        dA = self.loss_func.backward(y_pred, y_true)\n",
    "        \n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            if i < len(self.activations):\n",
    "                dZ = self.activations[i].backward(dA)\n",
    "            else:\n",
    "                dZ = dA\n",
    "            dA = self.layers[i].backward(dZ, self.learning_rate, self.reg_lambda)\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, verbose=True):\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            history.append(loss)\n",
    "            \n",
    "            # Backprop\n",
    "            self.backward(y_pred, y)\n",
    "            \n",
    "            if verbose and (epoch+1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        prob = self.forward(X)\n",
    "        return (prob >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d03250-357a-4284-960f-19b8d3dd147e",
   "metadata": {},
   "source": [
    "### 3. Data Preparation\n",
    "\n",
    "For demonstration, let's create a synthetic binary classification dataset. In a real project, you might load data from an external source, perform cleaning, normalizing, and potentially augmenting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7dc156-9fbf-4659-b1ed-f59b0e403954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffnn(scratch)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
